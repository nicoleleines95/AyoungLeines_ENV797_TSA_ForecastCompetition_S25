---
title: "M9: Advanced Forecasting Methods"
author: "Luana Lima"
output: pdf_document
always_allow_html: true
editor_options: 
  chunk_output_type: inline
  markdown: 
    wrap: sentence
---

## Learning Goals for M9

1.  Discuss challenges in forecasting higher frequency time series.
2.  Discuss complex seasonalities.
3.  Go over some functions in R that can handle multiple seasonal pattern.
4.  Develop a code for forecasting daily electricity demand.

## Setting R code chunk options

First R code chunk is used for setting the options for all R code chunks.
The choice echo=TRUE means both code and output will appear on report, include = FALSE neither code nor output is printed.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,tidy.opts=list(width.cutoff=80), tidy=FALSE) 
```

## Loading packages and initializing

Second R code chunk is for loading packages.
By setting message = FALSE and warning = FALSE, the code will appear but it will node include messages and warnings.

```{r package, message=FALSE, warning=FALSE}
library(lubridate)
library(ggplot2)
library(forecast)  
library(Kendall)
library(tseries)
library(outliers)
library(tidyverse)
library(smooth)
library(zoo)
library(kableExtra)
library(readxl)
```

```{r message=FALSE, warning=FALSE}
#Importing time series data from text file#
#temperature <- read_excel("../Data/temperature.xlsx")
#url <- "https://www.kaggle.com/competitions/tsa-spring-2025/data?select=load.xlsx"
#download.file(url, "load.xlsx")

load <- read_excel("../Data/load.xlsx")

# Convert the date column to Date format
load$date <- as.Date(load$date)
load$date <- ymd(load$date)

# Transform from wide to long format and then aggregate by meter_id and date
daily_data <- load %>%
  pivot_longer(cols = starts_with("h"), 
               names_to = "hour", 
               values_to = "demand") %>%
  group_by(meter_id, date) %>%
  summarise(daily_total = mean(demand, na.rm = TRUE), .groups = "drop")

summary(daily_data$daily_total)

p1 <-ggplot(daily_data, aes(x = date, y = daily_total)) +
  geom_line() +
  labs(title = "Daily data over time", x = "Date", y = "Daily demand") +
  theme_classic()

print(p1)
```


## Transforming both series into time series objects

Let's use `msts` on our two aggregated data sets.
`msts` stands for **m**ultiple **s**easonal **t**ime **s**eries.
The `msts()` function takes the following main arguments.

-   **data** numeric vector, matrix or data frame\
-   **seasonal.periods** vector with the seasonal periods\
-   **ts.frequency** seasonal period that should be used as frequency of the underlying ts object. The default value is max(seasonal.periods)\
-   **start** starting date/time of the time series. Now instead of only using start=`c(year, month)` we will use for example `start=c(year,month,day)`

Recall from `ts()` function that frequency is the number of observation per season.

```{r message=FALSE, warning=FALSE}


ts_daily_data <- msts(daily_data$daily_total, 
                           seasonal.periods =c(7,365.25),
                           start=c(2005,1,1))



```

## Decomposing both time series objects

So far we used `decompose()` and `stl()` to decompose time series in seasonal, trend and random.
For higher frequency data we will use a variation of `stl()`, the `mstl()` that can handle `msts` objects.

Since we are only analyzing the components and don't need to store in our environment, let's use pipes to generate the decomposition plot.

```{r message=FALSE, warning=FALSE}


ts_daily_data %>% mstl() %>%
  autoplot()

```

For the hourly data note three seasonal components based on the frequency we specified in the `msts` object.
To interpret this graph look at the vertical scales.
The trend has relatively narrow range compared to the other components, because there is little trend seen in the data.
As for the seasonal components still looking at vertical scales, yearly and daily pattern are stronger than weekly seasonality.

For the daily data both weekly and yearly pattern have strong scales and trend components.
Once again trend has a narrow range compared to seasonal components.

## Forecasting Daily Active Power

Now that we have the aggregated and tidy data we can start fitting models and forecasting.
For the purpose of this lesson we will work with the daily series.
*You can work on your own with the hourly series*.

Start by creating a subset of the time series to fit the models and leave observations out to check performance of the model.
We will set aside a year of data out-of-sample analysis.

```{r message=FALSE, warning=FALSE}
#create a subset for training purpose
n_for = 365
ts_daily_data_train <- subset(ts_daily_data,
                                   end = length(ts_daily_data)-n_for)

#create a subset for testing purpose
ts_daily_data_test <- subset(ts_daily_data,
                                   start = length(ts_daily_data)-n_for)

autoplot(ts_daily_data_train)
autoplot(ts_daily_data_test)
```

### Model 1: STL + ETS

Forecasts of `stl` objects can be obtained using function `stlf()`.
Forecast will be obtained by applying a non-seasonal exponential smoothing model to the seasonally adjusted data and re-seasonalizing using the last year of the seasonal component (for all seasonal components).

The model used with `stlf` is a ETS(.,.,.) model.
ETS stands for **E**rror, **T**rend, **S**easonal and can also be be thought of as **E**xponen**T**ial **S**moothing.

Possibilities for each component: Error={A,M}, Trend={N,A,M} and Seasonal = {N,A,M}.
Where N = none, A = additive and M = multiplicative.
R will **automatically** find the best fit for each component.

For more info and ETS equations refer to this [link](https://otexts.com/fpp2/ets.html).

Other functions to explore: `es()`, `ets()`, `hw()`

```{r ETS, echo=TRUE, message=FALSE, warning=FALSE}
#Fit and forecast STL + ETS model to data
ETS_fit <-  stlf(ts_act_power_daily_train,h=365)

#Plot foresting results
autoplot(ETS_fit) + ylab("Active Power")

#Plot model + observed data
autoplot(ts_act_power_daily) +
  autolayer(ETS_fit, series="STL + ETS",PI=FALSE) +
  ylab("Active Power")

#alternative coding if you don't need to save the objects
#ts_act_power_daily_train %>% stlf(h=365) %>% autoplot() 
#very easy to read/run/implement 
#but you don't have anything stored on your environment

```

### Model 2: ARIMA + FOURIER terms

Since we have multiple seasonalities, the SARIMA model will not work.
But we can work with an ARIMA model with Fourier terms for each seasonal period.
This models is known as a dynamic harmonic regression model with an ARMA error structure.
We will use a log transformation (`lambda=0`) in the `auto.arima()` to ensure the forecasts and prediction intervals remain positive.
And for the fourier terms we will use function `fourier()` from package `forecast`.
The number of fourier terms needed is specified in argument `K=`.
K should be a vector of integers specifying the number of sine and cosine terms for each of the seasonal periods.
Ideally K is chosen to minimize the AICc, but we will not go over it.

Fourier terms are used to model the seasonal components.
It resembles the seasonal dummies we used in previous modules, but they are based on trigonometric functions.
More info on fourier terms [here](https://otexts.com/fpp2/useful-predictors.html#useful-predictors).

```{r ARIMA, echo=TRUE, message=FALSE, warning=FALSE}
#Fit arima model with fourier terms as exogenous regressors
# seasonal = FALSE is the same as P=D=Q=0
# play with K by changing it to K=c(2,2), K=c(2,4), K=c(2,6), etc. The higher teh K the longer it will take to converge, because R will try more models.

ARIMA_Four_fit <- auto.arima(ts_act_power_daily_train, 
                             seasonal=FALSE, 
                             lambda=0,
                             xreg=fourier(ts_act_power_daily_train, 
                                          K=c(2,12))
                             )

#Forecast with ARIMA fit
#also need to specify h for fourier terms
ARIMA_Four_for <- forecast(ARIMA_Four_fit,
                           xreg=fourier(ts_act_power_daily_train,
                                        K=c(2,12),
                                        h=365),
                           h=365
                           ) 

#Plot foresting results
autoplot(ARIMA_Four_for) + ylab("Active Power")

#Plot model + observed data
autoplot(ts_act_power_daily) +
  autolayer(ARIMA_Four_for, series="ARIMA_FOURIER",PI=FALSE) +
  ylab("Active Power")

```

### Model 3: TBATS

BATS is Exponential smoothing state space model with **B**ox-Cox transformation, **A**RMA errors, **T**rend and **S**easonal components.
TBATS is a trigonometric seasonal variation of BATS.
A Box Cox transformation is a transformation of non-normal dependent variables into a normal shape.

More info on BATS and TBATS with corresponding equations [here](https://forecasters.org/wp-content/uploads/gravity_forms/7-2a51b93047891f1ec3608bdbd77ca58d/2014/06/BlaconÃ¡_MT_ISF2014.pdf.pdf).

```{r TBATS, echo=TRUE, message=FALSE, warning=FALSE}
# TBATS can take time to fit
TBATS_fit <- tbats(ts_act_power_daily_train)

TBATS_for <- forecast(TBATS_fit, h=365)

#Plot foresting results
autoplot(TBATS_for) +
  ylab("Active Power") 

#Plot model + observed data
autoplot(ts_act_power_daily) +
  autolayer(TBATS_for, series="TBATS",PI=FALSE)+
  ylab("Active Power") 

```

### Model 4: Neural Network Time Series Forecasts

There is a function in package `forecast` called `nnetar()` that will fit a feed-forward neural networks model to a time series.

A feed-forward neural network is fitted with lagged values of the series as inputs.
The inputs are for lags 1 to p, and lags s to sP where `s=frequency(y)`.
If xreg is provided, its columns are also used as inputs.
The network is trained for one-step forecasting.
Multi-step forecasts are computed recursively.

For non-seasonal data, the fitted model is denoted as an NNAR(p,k) model, where k is the number of hidden nodes.
This is analogous to an AR(p) model but with nonlinear functions.
For seasonal data, the fitted model is called an NNAR(p,P,k)[m] model, which is analogous to an ARIMA(p,0,0)(P,0,0)[s] model but with nonlinear functions.

```{r NNETAR, echo=TRUE, message=FALSE, warning=FALSE}
#You can play with the different values for p and P, you can also use xreg with Fourier term to model the multiple seasonality

#NN_fit <- nnetar(ts_act_power_daily_train,p=1,P=1)
NN_fit <- nnetar(ts_act_power_daily_train,
                 p=1,
                 P=0,
                 xreg=fourier(ts_act_power_daily_train, K=c(2,12)))

#NN_for <- forecast(NN_fit, h=365) 
NN_for <- forecast(NN_fit, 
                   h=365,
                   xreg=fourier(ts_act_power_daily_train, 
                                          K=c(2,12),h=365))


#Plot foresting results
autoplot(NN_for) +
  ylab("Active Power") 


#Plot model + observed data
autoplot(ts_act_power_daily) +
  autolayer(NN_for, series="Neural Network",PI=FALSE)+
  ylab("Active Power") 

```

## Checking accuracy of the fo models

```{r}

#Model 1: STL + ETS
ETS_scores <- accuracy(ETS_fit$mean,ts_act_power_daily_test)  

#Model 2: ARIMA + Fourier 
ARIMA_scores <- accuracy(ARIMA_Four_for$mean,ts_act_power_daily_test)

# Model 3:  TBATS 
TBATS_scores <- accuracy(TBATS_for$mean,ts_act_power_daily_test)

# Model 4:  Neural Network 
NN_scores <- accuracy(NN_for$mean,ts_act_power_daily_test)


```

Note that a new accuracy measure appeared in the table.
Theil's U is a relative accuracy measure that compares the forecasted results with the results of forecasting with minimal historical data.

### Compare performance metrics

Now we will create a data frame that combines performance metrics for all the three models.
You can choose one metric to help you choose among models.

```{r}
#create data frame
scores <- as.data.frame(
  rbind(ETS_scores, ARIMA_scores, TBATS_scores, NN_scores)
  )
row.names(scores) <- c("STL+ETS", "ARIMA+Fourier","TBATS","NN")

#choose model with lowest RMSE
best_model_index <- which.min(scores[,"RMSE"])
cat("The best model by RMSE is:", row.names(scores[best_model_index,]))                       
```

If you want generate a table to compare model accuracy and help visualize the results here is a suggestion on how to include a table on your Rmd report.
You can use the `kable_styling(latex_options="striped")` to highlight the model that leads to minimum RMSE.

```{r echo=FALSE, message=FALSE, warning=FALSE}
kbl(scores, 
      caption = "Forecast Accuracy for Daily Active Power",
      digits = array(5,ncol(scores))) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = "hold_position") %>%
  #highlight model with lowest RMSE
  kable_styling(latex_options="striped", stripe_index = which.min(scores[,"RMSE"]))
```

TBATS is the best model by RMSE, but ARIMA + Fourier is the best by MAPE.
Which one to choose when that is the case?

### Plotting everything together

Here we will use autoplot() and autolayer() from package `ggplot2` to draw a particular plot for time series.
The function autolayer() takes a few main arguments.

-   **x** Forecast object produced by forecast() function.If forecasts were generated with another function you may need to point to the object either mean or forecast to get the values.\
-   **include** number of values from time series to include in plot.Default is all values.\
-   **PI** Logical flag indicating whether to plot prediction intervals.\
-   **series** Matches an unidentified forecast layer with a colored object on the plot.

```{r}
autoplot(ts_act_power_daily) +
  autolayer(ETS_fit, PI=FALSE, series="STL+ETS") +
  autolayer(ARIMA_Four_for, PI=FALSE, series="ARIMA + Fourier") +
  autolayer(TBATS_for,PI=FALSE, series="TBATS") +
  autolayer(NN_for,PI=FALSE, series="NN") +
  xlab("Day") + ylab("Daily Active Power") +
  guides(colour=guide_legend(title="Forecast"))
```

If you want a closer look on last year just change the xlab range.

```{r}
autoplot(ts_act_power_daily_test) +
  autolayer(ETS_fit, PI=FALSE, series="STL+ETS") +
  autolayer(ARIMA_Four_for, PI=FALSE, series="ARIMA + Fourier") +
  autolayer(TBATS_for,PI=FALSE, series="TBATS") +
  autolayer(NN_for,PI=FALSE, series="NN") +
  ylab("Daily Active Power") +
  guides(colour=guide_legend(title="Forecast"))
```

## Additional suggestions for the competition

-   Try including just one seasonal period, i.e., 7.
    Since we just have a few years of historical data, we may not have enough to model the influence of season of the year on demand.

-   You noticed that most of the models we learned can handle multiple seasonality as long as we add Fourier terms as regressors.
    Play with values of K.

-   You may use `xreg=` to incorporate other exogenous variable like temperature and humidity.

-   Try combining the models with a simple average of the forecasts and check the accuracy metrics.
    If that leads to good model, try changing the weights when calculating the average.
